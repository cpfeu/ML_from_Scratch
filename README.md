# Machine Learning Algorithms Implemented from Scratch

## Softmax Classifier

Implementation of a shallow one layer neural network trained on the 
Iris flower dataset. The implementation uses stochastic gradient descent
with mini-batches and momentum to minimize the softmax cross-entropy 
loss with L2 weight decay regularization. After training, 
the linear decision boundaries are plotted.

## Regression

Implementation of Ridge Regression, Lasso Regression and 
Poisson Regression with the Million Song Dataset (MSD). Stochastic 
gradient descent as well as the closed form solutions are 
implemented. 

## Naive Bayes

Implementation of a naive bayes classifier that uses the Gaussian or Bernoulli
distribution to calculate class conditionals. This implementation was trained
on the *NLP with disasters Tweet* dataset from Kaggle for which the Bernoulli distribution should be used.
